# coding:utf-8
import json
import os
import time

import gradio as gr
import torch
from modelscope.hub.snapshot_download import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from transformers.generation.utils import GenerationConfig

cache_dir = './'
MODEL_PATH = 'X-D-Lab/MindChat-Qwen-7B'

# snapshot_download(MODEL_PATH,
#                   cache_dir=cache_dir,
#                   revision='v1.0.0')

tokenizer = AutoTokenizer.from_pretrained(cache_dir +
                                        MODEL_PATH, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained(cache_dir +
#                                           MODEL_PATH, device_map="auto", trust_remote_code=True, fp16=True).eval()

# ---------------------------------------------------
# é‡åŒ–ä¸ºint8
nf4_config = BitsAndBytesConfig(
    load_in_4bit=False,
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf8"
)
model = AutoModelForCausalLM.from_pretrained(cache_dir + MODEL_PATH,
                                             device_map="auto",  # Use Intel CPU for inference
                                             torch_dtype=torch.float16,
                                             trust_remote_code=True,
                                             resume_download=True,
                                             quantization_config=nf4_config)
# ----------------------------------------------



model.generation_config = GenerationConfig.from_pretrained(cache_dir +
                                          MODEL_PATH, trust_remote_code=True)


title = "ğŸ‹MindChat: æ¼«è°ˆå¿ƒç†å¤§æ¨¡å‹"

description = """
ğŸ” MindChat(æ¼«è°ˆ): æ—¨åœ¨é€šè¿‡è¥é€ è½»æ¾ã€å¼€æ”¾çš„äº¤è°ˆç¯å¢ƒ, ä»¥æ”¾æ¾èº«å¿ƒã€äº¤æµæ„Ÿå—æˆ–åˆ†äº«ç»éªŒçš„æ–¹å¼, ä¸ºç”¨æˆ·æä¾›éšç§ã€æ¸©æš–ã€å®‰å…¨ã€åŠæ—¶ã€æ–¹ä¾¿çš„å¯¹è¯ç¯å¢ƒ, ä»è€Œå¸®åŠ©ç”¨æˆ·å…‹æœå„ç§å›°éš¾å’ŒæŒ‘æˆ˜, å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•.

ğŸ¦Š æ— è®ºæ˜¯åœ¨å·¥ä½œåœºæ‰€è¿˜æ˜¯åœ¨ä¸ªäººç”Ÿæ´»ä¸­, MindChatæœŸæœ›é€šè¿‡è‡ªèº«çš„åŠªåŠ›å’Œä¸“ä¸šçŸ¥è¯†, åœ¨ä¸¥æ ¼ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹, å…¨æ—¶æ®µå…¨å¤©å€™ä¸ºç”¨æˆ·æä¾›å…¨é¢çš„å¿ƒç†é™ªä¼´å’Œå€¾å¬, åŒæ—¶å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•, ä»¥æœŸä¸ºå»ºè®¾ä¸€ä¸ªæ›´åŠ å¥åº·ã€åŒ…å®¹å’Œå¹³ç­‰çš„ç¤¾ä¼šè´¡çŒ®åŠ›é‡.

ğŸ™…â€ ç›®å‰ï¼ŒMindChatè¿˜ä¸èƒ½æ›¿ä»£ä¸“ä¸šçš„å¿ƒç†åŒ»ç”Ÿå’Œå¿ƒç†å’¨è¯¢å¸ˆï¼Œæ— æ³•åšå‡ºä¸“ä¸šçš„å¿ƒç†è¯Šæ–­æŠ¥å‘Šã€‚è™½MindChatåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æè‡´æ³¨é‡æ¨¡å‹å®‰å…¨å’Œä»·å€¼è§‚æ­£å‘å¼•å¯¼ï¼Œä½†ä»æ— æ³•ä¿è¯æ¨¡å‹è¾“å‡ºæ­£ç¡®ä¸”æ— å®³ï¼Œå†…å®¹ä¸Šæ¨¡å‹ä½œè€…åŠå¹³å°ä¸æ‰¿æ‹…ç›¸å…³è´£ä»»ã€‚

ğŸ‘ æ›´ä¸ºä¼˜è´¨ã€å®‰å…¨ã€æ¸©æš–çš„æ¨¡å‹æ­£åœ¨èµ¶æ¥çš„è·¯ä¸Šï¼Œæ¬¢è¿å…³æ³¨ï¼š[MindChat Github](https://github.com/X-D-Lab/MindChat)
"""

submit_btn = 'å‘é€'
retry_btn = 'ğŸ”„ é‡æ–°ç”Ÿæˆ'
undo_btn = 'â†©ï¸ æ’¤é”€'
clear_btn = 'ğŸ—‘ï¸ æ¸…é™¤å†å²'


def predict(message, history):

    if history is None:
        history = []
    history = history[-20:]
    response, history = model.chat(tokenizer, message, history=history) 

    history.append((message, response))

    for i in range(len(response)):
        time.sleep(0.02)
        yield response[:i + 1]


demo = gr.ChatInterface(predict,
                        title=title,
                        description=description,
                        cache_examples=True,
                        submit_btn=submit_btn,
                        retry_btn=retry_btn,
                        clear_btn=clear_btn,
                        undo_btn=undo_btn).queue()

demo.launch()